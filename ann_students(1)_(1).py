# -*- coding: utf-8 -*-
"""ANN_students(1) (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tyNW6jwlntHfKNHbxrRZaflJtO2-BfYz
"""

# Solo en google colab
#Cambia el entorno de ejecución a TPU

# Solo en google colab
# Instalar tensorflow_addons
!pip install tensorflow_addons

import numpy as np
import pandas as pd
import tensorflow as tf
import tensorflow_addons as tfa
from tensorflow import keras #es una libreria
from tensorflow.keras import layers
import matplotlib as plt
import seaborn as sns
sns.set(style="darkgrid")

"""## Tensorflow

¿Cuál es valor de $w$ que minimiza el valor de la siguiente función siendo $x = [100, -5, 2]$?

$$x_0 + x_1w + x_2w^2$$

#### Solución
$w=5/4$

### Definición del modelo
"""

# Coeficientes a optimizar
w = tf.Variable(0, name='w', dtype=tf.float32) #definido la variable
# Función a optimizar/minimizar
f = lambda: 2.*w**2. -5.*w + 100. #definido la función a minimizar

# Optimizador de tipo descenso de gradiente estocástico
optimizer = tf.optimizers.SGD(learning_rate=0.1) #algoritmo con los que voy a dar los saltos, SGD descenso del gradiente
#learning_rate no puede ser ni muy grande ni muy pequeño
# Iteramos 20 veces el optimizador
for i in range(20):
    optimizer.minimize(f,(w,))
    print("iterations: ", i, ",\f: ", f().numpy(), ",\tw: ", w.numpy())

"""## Perceptrón simple: Compuertas AND, OR y XOR"""

# Entradas a la compuerta
X = np.array([[0.,0.],[0.,1.],[1.,0.],[1.,1.]], dtype=np.float32)

y_and = np.array([[0.],[0.],[0.],[1.]], dtype=np.float32)

y_or = np.array([[0.],[1.],[1.],[1.]], dtype=np.float32)

y_xor = np.array([[0.],[1.],[1.],[0.]], dtype=np.float32)

# Definimos los pesos y bias
W = tf.constant([[0.], [0.]], name='w', dtype=tf.float32)
b = tf.constant(0., name='b', dtype=tf.float32)

# Perceptrón simple (con función de activación sigmoide)
y_ = tf.nn.sigmoid(tf.matmul(X,W, name='mat_mul') + b, name='sigmoide')

# Resultado
print("Output", np.round(y_.numpy()).reshape(4,))

"""#### Probemos ahora optimizando con tensorflow"""

# Definimos los pesos, ahora son variables porque tenemos que optimizarlos
W = tf.Variable(tf.random.normal(shape=[2,1],stddev=1./np.sqrt(2)), name='w', dtype=tf.float32)
b = tf.Variable(tf.zeros(1), name='b', dtype=tf.float32)

#Definimos las métricas
cost = tf.metrics.Mean(name='cost')
accuracy = tf.metrics.BinaryAccuracy(name='accuracy')

#Definimos el optimizador (SGD)
optimizer = tf.optimizers.SGD(learning_rate=0.25)

@tf.function
def train_step(X, y):
    with tf.GradientTape() as tape:
        # Se registran las funciones a optimizar
        y_ = tf.nn.sigmoid(tf.matmul(X,W, name='mat_mul') + b, name='sigmoide')
        # Usar binary_crossentropy
        loss = tf.losses.binary_crossentropy(y,y_)
    #Obtenemos los gradientes
    gradients = tape.gradient(loss, (W, b))
    #Optimizamos (1 paso)
    optimizer.apply_gradients(zip(gradients, (W, b)))

    #Calculamos las métricas
    cost(loss)
    accuracy(y, y_)

"""Probemos el funcionamiento de nuetro perceptron simple con las tres compuertas
#### AND
"""

EPOCHS = 20

for epoch in range(EPOCHS):
    # Reset the metrics at the start of the next epoch
    cost.reset_states()
    accuracy.reset_states()

    train_step(X, y_and)

    template = 'Epoch {}, Cost: {}, Accuracy: {}'
    print(template.format(epoch+1,
                        cost.result(),
                        accuracy.result()*100))

y_ = tf.nn.sigmoid(tf.matmul(X, W, name='matmul') + b, name="sigmoid")
print("Output", np.round(y_.numpy()).reshape(4,))

"""#### OR
Ejecutar primero el bloque donde se definen los pesos para reinicializar los valores
"""

EPOCHS = 20

for epoch in range(EPOCHS):
    # Reset the metrics at the start of the next epoch
    cost.reset_states()
    accuracy.reset_states()

    train_step(X, y_or)

    template = 'Epoch {}, Cost: {}, Accuracy: {}'
    print(template.format(epoch+1,
                        cost.result(),
                        accuracy.result()*100))

y_ = tf.nn.sigmoid(tf.matmul(X, W, name='matmul') + b, name="sigmoid")
print("Output", np.round(y_.numpy()).reshape(4,))

"""#### XOR
Ejecutar primero el bloque donde se definen los pesos para reinicializar los valores
"""

EPOCHS = 20

for epoch in range(EPOCHS):
    # Reset the metrics at the start of the next epoch
    cost.reset_states()
    accuracy.reset_states()

    train_step(X, y_xor)

    template = 'Epoch {}, Cost: {}, Accuracy: {}'
    print(template.format(epoch+1,
                        cost.result(),
                        accuracy.result()*100))

y_ = tf.nn.sigmoid(tf.matmul(X, W, name='matmul') + b, name="sigmoid")
print("Output", np.round(y_.numpy()).reshape(4,))

"""#### ¿Qué ha sucedido con la compuerta XOR?

## Perceptrón Multicapa: Compuerta XOR
Crear un perceptron multicapa con topología 2-2-1 (2 entradas, 2 ocultas y una salida)
"""

# Pesos del perceptrón multicapa
Wh = tf.Variable(tf.random.normal(shape=[2,2],stddev=1./np.sqrt(2)), name='wh', dtype=tf.float32)
bh = tf.Variable(tf.zeros(2), name='bh', dtype=tf.float32)

Wo = tf.Variable(tf.random.normal(shape=[2,1],stddev=1./np.sqrt(2)), name='wh', dtype=tf.float32)
bo = tf.Variable(tf.zeros(1), name='bh', dtype=tf.float32)

#Definimos las métricas
cost = tf.metrics.Mean(name='cost')
accuracy = tf.metrics.BinaryAccuracy(name='accuracy')

#Definimos el optimizador (SGD)
optimizer = tf.optimizers.SGD(learning_rate=0.42)

@tf.function
def train_step(X, y):
    with tf.GradientTape() as tape:
        # Se registran las funciones a optimizar
        # Perceptrón multicapa
        #Capa oculta, usa la tangente hiberbolica porque es flexible que me permite usar variabilidad
        a_h = tf.nn.tanh(tf.matmul(X, Wh) + bh, name='a_h')
        #Capa de salida, sigmoide porque es de clasificacion binario
        y_ = tf.nn.sigmoid(tf.matmul(a_h, Wo) + bo, name='y_')
        # Usar binary_crossentropy
        loss = tf.losses.binary_crossentropy(y,y_)
    #Obtenemos los gradientes
    gradients = tape.gradient(loss, (Wh, bh, Wo, bo))
    #Optimizamos (1 paso)
    optimizer.apply_gradients(zip(gradients, (Wh, bh, Wo, bo)))

    #Calculamos las métricas
    cost(loss)
    accuracy(y,y_)

"""Ejecuta varias veces junto al anterior, no encuentra siempre la solución"""

EPOCHS = 100

for epoch in range(EPOCHS):
    # Reset the metrics at the start of the next epoch
    cost.reset_states()
    accuracy.reset_states()

    train_step(X, y_xor)
    if epoch%10==0:
        template = 'Epoch {}, Cost: {}, Accuracy: {}'
        print(template.format(epoch+1,
                            cost.result(),
                            accuracy.result()*100))

y_ = tf.nn.sigmoid(tf.matmul(tf.nn.tanh(tf.matmul(X, Wh) + bh, name='a_h'), Wo) + bo, name='y_')
print("Output", np.round(y_.numpy()).reshape(4,))

"""## Problema regresión: Diabetes
## TENEMOS QUE HACERLO NOSOTRAS
"""

import io
import requests


# https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html
# age, sex, body mass index, average blood pressure, and six blood serum measurements
url="https://www4.stat.ncsu.edu/~boos/var.select/diabetes.tab.txt"
c=requests.get(url).content
diabetes=pd.read_csv(io.StringIO(c.decode('utf-8')), sep='\t')

diabetes.head()

from sklearn.model_selection import train_test_split

diabetes['SEX'] -= 1
X_train, X_test, y_train, y_test = train_test_split(diabetes.drop('Y', axis=1).values,
                                                    #Aniadimos una dimension para que sea un vector columna
                                                    diabetes['Y'].values[..., np.newaxis],
                                                    test_size=0.2, random_state=0)

from sklearn.preprocessing import StandardScaler

class StandardScalerSelection(StandardScaler):
    '''Solo funciona con pandas '''

    def __init__(self, copy=True, with_mean=True, with_std=True, skip_cols=None):
        self.skip_cols = np.array(skip_cols).flatten().tolist()
        super().__init__(copy, with_mean, with_std)

    def fit(self, X, y=None):
        return super().fit(np.delete(X, self.skip_cols, axis=1))

    def transform(self, X):
        X_scl = super().transform(np.delete(X, self.skip_cols, axis=1))
        # Restar uno por cada indice anterior para que asi se inserte en su posición
        idx = np.array(skip_cols)-np.arange(len(skip_cols))
        return np.insert(X_scl, idx, X[:,self.skip_cols], axis=1)

skip_cols = sorted([diabetes.columns.get_loc(c) for c in ['SEX'] if c in diabetes])
scaler = StandardScalerSelection(skip_cols=skip_cols)
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
y_train = y_train.astype(np.float64)
y_test = y_test.astype(np.float64)

"""### TensorFlow"""

#TODO
# Pesos del perceptrón multicapa
Wh1 = #TODO
bh1 = #TODO

Wh2 = #TODO
bh2 = #TODO

Wo = #TODO
bo = #TODO

#Definimos las funciones de coste
train_cost = #TODO
train_r2 = #TODO
test_cost = #TODO
test_r2 = #TODO


#Definimos el optimizador (Adam)
optimizer = #TODO

@tf.function
def train_step(X, y):
    #TODO

# Función para obtener la precision y coste en test
@tf.function
def test(X, y):
    # Perceptrón multicapa
    #TODO
    # Funcion de perdida
    #TODO
    #Calculamos las métricas
    #TODO

EPOCHS = 100

for epoch in range(EPOCHS):
    # Reset the metrics at the start of the next epoch
    train_cost.reset_states()
    train_r2.reset_states()
    test_cost.reset_states()
    test_r2.reset_states()

    train_step(X_train, y_train)
    test(X_test, y_test)
    if epoch%10==0:
        template = 'Epoch {:}, Train cost: {:.4f}, Train R2: {:.4f}, Test cost: {:.4f}, Test R2: {:.4f}'
        print(template.format(epoch+1,
                            train_cost.result(),
                            train_r2.result(),
                            train_cost.result(),
                            test_r2.result()))

"""## Keras"""

#Modelo
#64 es el número de neuronas en la primera capa oculta, input_shape es la cantidad de variables que van a entrar, es decir, el número de columnas que tenga X_train
model = keras.Sequential([
    layers.Dense(64, activation=tf.nn.tanh, input_shape=[X_train.shape[1]], name='h1'),
    layers.Dense(32, activation=tf.nn.tanh, name='h2'),
    layers.Dense(1, name='o'),
  ])
#loss es la funcion de perdida,
model.compile(loss = 'mean_squared_error',
              optimizer=tf.keras.optimizers.Adam(0.1),
              metrics=['mean_absolute_error','mean_squared_error'])

model.summary()

# Entrenar el modelo
history = model.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=1)

# Obtener la precisión del modelo, evaluamos el modelo
model.evaluate(X_test, y_test)

hist = pd.DataFrame(history.history)
# Incluye la epoca para cada error
hist['epoch'] = history.epoch

# Colpasa los errores 'mean_absolute_error','val_mean_absolute_error' en una sola columna para sns
df = hist.melt(id_vars='epoch', var_name='Type', value_name='Error',
               value_vars=['mean_absolute_error','val_mean_absolute_error'])
# Plot the responses for different events and regions
_ = sns.lineplot(x='epoch', y='Error', hue='Type', data=df)


print("Mean Absolute Error Train: %.2f" % model.evaluate(X_train, y_train, verbose=0)[1])
print("Mean Absolute Error Test: %.2f" % model.evaluate(X_test, y_test, verbose=0)[1])

"""### Problema de clasificación: Breast Cancer
# Es como la deteccion de fraude, y en la ultima capa vamos a necesitar una funcion de activacion sigmoide
"""

import io
import requests

# https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)
url='https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data'
c=requests.get(url).content
cancer=pd.read_csv(io.StringIO(c.decode('utf-8')), header=None, index_col=0)

cancer.head()

from sklearn.model_selection import train_test_split

cancer[1] = list(map(float, cancer[1].values=='M'))
X_train, X_test, y_train, y_test = train_test_split(cancer.drop(1, axis=1).values,
                                                    #Aniadimos una dimension para que sea un vector columna
                                                    cancer[1].values[..., np.newaxis],
                                                    test_size=0.2, random_state=0)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""### TensorFlow
Utilizar en este caso la métrica `AUC`

Para la función de perdida, utilizar `binary_crossentropy` al ser un problema binario
"""

#TODO
# Pesos del perceptrón multicapa
#TODO

#Definimos las métricas
train_cost = #TODO
train_auc = #TODO
test_cost = #TODO
test_auc = #TODO


#TODO...

EPOCHS = 100

for epoch in range(EPOCHS):
    # Reset the metrics at the start of the next epoch
    train_cost.reset_states()
    train_auc.reset_states()
    test_cost.reset_states()
    test_auc.reset_states()

    train_step(X_train, y_train)
    test(X_test, y_test)
    if epoch%10==0:
        template = 'Epoch {:}, Train Cost: {:.4f}, Train AUC: {:.2f}, Test Cost: {:.4f}, Test AUC: {:.2f}'
        print(template.format(epoch+1,
                            train_cost.result(),
                            train_auc.result()*100,
                            test_cost.result(),
                            test_auc.result()*100))

"""### Keras"""

#TODO

hist = pd.DataFrame(history.history)
# Incluye la epoca para cada error
hist['epoch'] = history.epoch

# Colpasa los errores 'mean_absolute_error','val_mean_absolute_error' en una sola columna para sns
df = hist.melt(id_vars='epoch', var_name='Type', value_name='Accuracy',
               value_vars=['binary_accuracy','val_binary_accuracy'])
# Plot the responses for different events and regions
_ = sns.lineplot(x='epoch', y='Accuracy', hue='Type', data=df)

print("Accuracy Train: %.2f" % model.evaluate(X_train, y_train, verbose=0)[1])
print("Accuracy Test: %.2f" % model.evaluate(X_test, y_test, verbose=0)[1])

"""## Problema de clasificación multiclase: MNIST"""

(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()

X_train = train_images / 255.0
X_test = test_images / 255.0

# OneHotEncoding para AUC
from sklearn.preprocessing import LabelBinarizer

le = LabelBinarizer().fit(train_labels)
y_train_label = le.transform(train_labels)
y_test_label = le.transform(test_labels)

y_train = train_labels[..., np.newaxis]
y_test = test_labels[..., np.newaxis]

"""### Tensorflow

`sparse_categorical_crossentropy` funciona mejor en problemas donde las clases son mutuamente excluyentes, es decir, solo hay una clase positiva en cada tupla. `categorical_crossentropy` permite trabajar con problemas donde más de una clase puede ser positiva.
"""

#TODO

EPOCHS = 10

for epoch in range(EPOCHS):
    # Reset the metrics at the start of the next epoch
    train_cost.reset_states()
    train_auc.reset_states()
    test_cost.reset_states()
    test_auc.reset_states()

    train_step(X_train, y_train, y_train_label)
    test(X_test, y_test, y_test_label)
    if epoch%1==0:
        template = 'Epoch {:}, Train Cost: {:.4f}, Train AUC: {:.2f}, Test Cost: {:.4f}, Test AUC: {:.2f}'
        print(template.format(epoch+1,
                            train_cost.result(),
                            train_auc.result()*100,
                            test_cost.result(),
                            test_auc.result()*100))

"""### Keras
Probar a crear el modelo con y sin dropout y batch normalization para comparar los resultados de validación
"""

#TODO

hist = pd.DataFrame(history.history)
# Incluye la epoca para cada error
hist['epoch'] = history.epoch

# Colpasa los errores 'mean_absolute_error','val_mean_absolute_error' en una sola columna para sns
df = hist.melt(id_vars='epoch', var_name='Type', value_name='Accuracy',
               value_vars=['accuracy','val_accuracy'])
# Plot the responses for different events and regions
_ = sns.lineplot(x='epoch', y='Accuracy', hue='Type', data=df)

print("Accuracy Train: %.2f" % model.evaluate(X_train, y_train, verbose=0)[1])
print("Accuracy Test: %.2f" % model.evaluate(X_test, y_test, verbose=0)[1])